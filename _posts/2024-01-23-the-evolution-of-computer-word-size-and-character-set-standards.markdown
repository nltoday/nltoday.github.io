---
layout: post
title: "The Evolution of Computer Word Size and Character Set Standards"
date: 2024-01-23 13:42:05 +0000
categories: "History"
excerpt_image: https://i.ytimg.com/vi/ldxfE_SfRRU/maxresdefault.jpg
image: https://i.ytimg.com/vi/ldxfE_SfRRU/maxresdefault.jpg
---

### Standardizing for Interoperability
In the early days of computing, each computer manufacturer used their own unique character sets with limited printable characters. **First-generation computers** typically had word sizes of 12, 18, 36, or even 60 bits to reflect the technology of that time. There were also [decimal computers](https://store.fi.io.vn/work-hard-so-my-st-bernard-live-a-better-dog-lover-2) that performed arithmetic in decimal rather than binary. **Character arrays** were not widely supported yet. 
The need for interoperability drove the push for standards. When **IBM designed the IBM 7030** also known as **Stretch**, they took feedback from government agency customers on their requirements. The **CIA insisted on an 8-bit character** and IBM designed peripherals to match this standard. With 64-bit words, Stretch was also one of the first computers to use 8-bit bytes as its fundamental data unit.

![](http://d1oqwsnd25kjn6.cloudfront.net/production/files/117535/large_original/ToddMcLeod-073-WordSize-Screenshot.png?1429559470)
### Deriving Lessons from Stretch 
Although Stretch achieved its goal of being 100 times faster than previous machines, it only ended up being 50 times faster and was seen as a failure by IBM management. IBM used the lessons learned to design their **System/360 family** of computers with a **360-degree** design philosophy. 
System/360 established several important **industry standards**: it addressed bytes of 8 bits, had word sizes of 32 or 64 bits formed from 4 or 8 bytes, and instructions that were 2, 4 or 6 bytes long. The **24-bit address** space fit neatly within a 32-bit word. Other manufacturers like **Burroughs, Univac, NCR, CDC, and Honeywell** followed IBM's lead, knowing they set the standards that defined the **mainframe market**.
### Expanding Address Spaces 
As memory costs declined and capacities increased, there was a push to go beyond the 32-bit limits. DEC chose 64 bits for their **Alpha** and IBM, Motorola, and others agreed on 64 bits for the **PowerPC** architecture. Intel's failed **Itanium** chip was also 64-bit. AMD created the 64-bit extension to the **x86 instruction set** we know as **x86-64**. Now, 64-bit CPUs are ubiquitous in desktops and servers. 
Other architectures also expanded to 64-bits over time. The 32-bit **ARM** design now has a 64-bit version, reflecting the need for more addressable memory in modern SoCs and mobile devices.
### Growth of Personal Computing
In the microcomputer era, the **PDP-11** was influential with its 16-bit data path and 32-bit addressing provided by later models. Digital continued advancing memory capacities with the **32-bit VAX** architecture. As personal computers grew more powerful for mainstream users and developers, Intel introduced 32-bit extensions to the x86 with the **Pentium**. 
### Standardizing at 64 Bits
By the mid-1990s, the push was on for a **common 64-bit computing standard**. With Microsoft's endorsement, AMD's **x86-64** debuted in 2003 and became the **de facto standard** despite Intel's competitive efforts. The standardization around 64-bit addressing and 8-byte datatypes created a vast memory and performance “playing field” for modern operating systems, databases, and applications.
### Architectures Continue Evolving
Of course, change continues as technologies progresses. New **arm64** processors now power the latest smartphones, laptops, and servers while maintaining **backward compatibility**. Specialized computing models like **GPUs** have also advanced significantly in parallel with these trends. Looking ahead, initiatives like **CPU/RAM co-processors** and **quantum computing** promise even more drastic changes in how systems are architected and programmed going forward. But the foundations of 8-byte addressability, 64-bit computing, and backward compatibility remain critically important.
The evolution of computer word sizes and character set standards established the core foundations of modern computing. What started as unique characteristics of individual systems converged, through market forces and standardization, into the ubiquitous conventions that continue enabling interoperability and advancing technology today. Future innovations will undoubtedly change other aspects, but these fundamental building blocks seem destined to remain cornerstones for generations to come.
![The Evolution of Computer Word Size and Character Set Standards](https://i.ytimg.com/vi/ldxfE_SfRRU/maxresdefault.jpg)